---
title: "Handwritten Digits Recognition: STA 523 Final Project"
author: "Zheng Ding"
output: html_document
---

### Introduction
In this final project, we use Support Vector Machines (SVMs) to solve handwriting digits recognition problem. Support Vector Machine was originally designed and used for binary classification. However, it is also a great tool for multi-class classification, novelty detection, and regression, which requires to construct a series of binary classifiers. We used 'e1071' and 'kernlab' these two packages for SVMs with Gaussian Kernel Function. Then we did some visulization with respect to the testing results.


### Database
The data we used was from the Modified National Institute of Standards and Technology (MNIST) database, which is the most famous database of handwritten digits from real world examples. This database contains a total of 70000 observations in training and testing datasets, each observation is a desaturation 28 * 28 = 784 pixels image, and each pixel contains a grey-scale value from 0 to 255.
The only difference between training and testing data is that a training data contains a label with value of 0-9, which indicates the actual handwritten digit.

### Read in Data and Plot Digits
We first read in the data into R, and each row represents a handwritten digit. For training.data, it contains a label with a value 0-9, which indicates the actual handwritten digits.

```{r}
library(e1071)
library(caret)
library(kernlab)

## Read training and testing data sets
training.data <- read.csv("train.csv",header=T)
testing.data <- read.csv("test.csv",header=T)
```
As we mentioned above, this is a classification problem. Hence the label column should be treated as category variable. We used the 'as.factor' function in order to convert the label value from numeric to factor.

```{r}
## convert predicted column to factor
training.data$label <- as.factor(training.data$label)
```

By mapping the 784 grey-scale values back into a 28 * 28 matrix，we can actully plot out the digit images from the read in data. We picked 10 digits from the training data set.

```{r}
library(lattice)
library(gridExtra)
## plot sample digit 0
vec=c(2,39,23,66,82,100,161,173,215,87)
list.plot=rep(list(list()),10)
for (i in 1:10){
  digit=matrix(training.data[vec[i],2:785],nrow=28,ncol=28)
  digit=digit[,nrow(digit):1]
  plot=levelplot(digit,col.regions = grey(seq(1, 0, length = 256)),colorkey=FALSE)
  list.plot[[i]]=plot
}
grid.arrange(list.plot[[1]],list.plot[[2]],list.plot[[3]],list.plot[[4]],
             list.plot[[5]],list.plot[[6]],list.plot[[7]],list.plot[[8]],
             list.plot[[9]],list.plot[[10]],nrow=2,ncol=5)
```

### Training Accuracy Test
In order to assess the accuracy of the model, we used K-folds cross-validation. In K-fold cross-validation, the original sample is randomly partitioned into K equal size subsamples. Of the K subsamples, a single subsample is retained as the validation data for testing the model, and the remaining K-1 subsamples are used as training data. This cross-validation process is then repeated K times, with each of the K subsamples used exactly once as the validation data. For testing, we used cross validation for 5000 training data with K = 5 folds.

We used the 'createFolds' function from the 'caret' package in order to create K = 5 folds. 

```{r}
## take 5000 sample data for cross validation
cross.validation.training <- training.data[1:5000,]

## create 5 folds
num.folds <- 5
folds <- createFolds(cross.validation.training$label , k = num.folds, list = TRUE, returnTrain = FALSE)
```
Using 'ksvm' function from the 'kernlab' in order to build the svm model. As the label column was transformed into a factor class, the SVM model will use 'C-SVM classification' as the default type with Gaussian Kernel Function. The 'predict' function would be then used in order to predict the labels based on the testing set. 

In order to obtain the accuracy of the prediction, the 'confusionMatrix' function from the 'caret' package would be used as well.

```{r, warning=FALSE}
accuracy <- numeric(5)
for ( i in 1:length(folds)){
    
    ## Subset data to train/test sets
    cat("Creating training and testing data sets for fold num: ", i, "\n")
    train <- cross.validation.training[as.numeric(unlist(folds[-i])),]
    test <- cross.validation.training[as.numeric(unlist(folds[i])),]    
    
    ## Build SVM model
    cat("Buliding model for fold num: ", i, "\n")
    fit.svm <- ksvm(label~., data=train, kernel="rbfdot")
    
    ## Prediction for new data
    cat("Making prediction for fold num: ", i, "\n")
    predict.svm <- predict(fit.svm,test)
    
    ## calculate accuracy
    cat("Calculating accuracy for fold num: ", i, "\n")
    tmp <- confusionMatrix(test$label, predict.svm)
    overall <- tmp$overall
    cat(overall, "\n")
    overall.accuracy <- overall['Accuracy']
    cat(overall.accuracy, "\n")
    accuracy[i] <- as.numeric(tmp[[3]][1])
}
```

There may show some warning which occurring because there are many columns with zero value, meaning that the variable is constant, hence cannot be scaled to unit variance (and zero mean). However, it is not effecting the model.

The average accuracy we got from the k-fold cross validation was about 94.6%. The lowest accuracy was 93.3% and the highest was 95.1% in our tests. 

```{r}
paste("Avg accuracy: ",round(mean(accuracy)*100,1),"%",sep="")
```

### Visulization
```{r}
library(grDevices)
library(RColorBrewer)
table <- as.matrix(tmp$table)
table
```
In the table above, the columns represent the actual digits, and the rows represent predictions. Hence, the diagonal values are the numbers of correct predictions. For example, the value of the 4th row and 1st column is 1, which means that the function misread 0 as 3.

```{r}
byclass <- as.matrix(tmp$byClass)
colors <- colorRampPalette(brewer.pal(9, "Pastel1"))
plot1 <- barplot(byclass[,1], col = colors(9), main = "Correct Classification Rate", 
    ylab = "Digits",ylim=c(0,1.2))
text(plot1,0,labels = round(byclass[,1],3), cex=0.9,pos=3)
plot2 <- barplot(byclass[,8], col = colors(9), main = "Balanced Accuracy", 
    ylab = "Digits",ylim=c(0,1.2))
text(plot2,0,labels = round(byclass[,8],3), cex=0.9,pos=3)
```

When looking at the Sensitivity Rate (True Positive Rate), it seem that the model was performing a bit better for some specific classes. For classes 0, 1, 6 and 7, the model was to predict correctly in a rate above 95%. However, for other classes, the True Positive Rate was only within 90% - 92.5%.

We created another bar plot for the Balanced Accuracy, which is another useful performance measure that avoids inflated performance estimates on imbalanced datasets. After averaging sensitivity and specificity, we can see from the plot that the predictive accuracy accross the 10 digits are more similar.

### Prediction on Full Data
With overall accuracy of 94.6%, we than trained the model on the full training set and then predicted the label based on the full testing set.

The outcome of the prediction was written into a csv file, 'prediction.csv'.

```{r, warning=FALSE}
## run model on full data set
## fit svm model 
## using Gaussian kernel
fit.svm <- ksvm(label~., data=training.data, kernel="rbfdot")

## predict labels for testing data
predict.svm <- predict(fit.svm,testing.data)

## merge predictions for one data frame
prediction <- data.frame(label=predict.svm,testing.data)

## write results to file
write.csv(prediction,file="prediction.csv",row.names=F)
```

Since the test data do not have labels, it is not easy to verify the accuracy. However, we can still visual plot out the images and compare them with our predictions. We would not do all the cases for sure. In our 20 example cases, except the 4th digit '0' is misread as '9', the rest 19 predictions are correctly, which roughly gives a 95% accuracy as expected. This result is consistent with the average accuracy we calculated previously.

```{r}
## visualization of prediction
## 1 error out of 10 predictions

vec=seq(1:20)
list.plot=rep(list(list()),20)
for (i in 1:20){
  digit=matrix(prediction[vec[i],2:785],nrow=28,ncol=28)
  digit=digit[,nrow(digit):1]
  plot=levelplot(digit,col.regions = grey(seq(1, 0, length = 256)),colorkey=FALSE)
  list.plot[[i]]=plot
}
grid.arrange(list.plot[[1]],list.plot[[2]],list.plot[[3]],list.plot[[4]],
             list.plot[[5]],list.plot[[6]],list.plot[[7]],list.plot[[8]],
             list.plot[[9]],list.plot[[10]],nrow=2,ncol=5)
grid.arrange(list.plot[[11]],list.plot[[12]],
             list.plot[[13]],list.plot[[14]],list.plot[[15]],list.plot[[16]],
             list.plot[[17]],list.plot[[18]],list.plot[[19]],list.plot[[20]],nrow=2,ncol=5)
as.character(prediction[1:10,1])
as.character(prediction[11:20,1])
``` 